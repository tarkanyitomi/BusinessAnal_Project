{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb06b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stations_clusters_on_map(clustered_stations: pd.DataFrame):\n",
    "    # Create a map centered at the mean coordinates of all stations\n",
    "    center_lat = clustered_stations['latitude'].mean()\n",
    "    center_lon = clustered_stations['longitude'].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "    # Add markers for each station, colored by cluster\n",
    "    colors = ['#%06x' % np.random.randint(0, 0xFFFFFF) for _ in range(clustered_stations[\"cluster\"].nunique())]\n",
    "    for _, row in clustered_stations.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=5,\n",
    "            color=colors[int(row[\"cluster\"])] if row[\"cluster\"] != -1 else 'gray',\n",
    "            fill=True,\n",
    "            popup=f\"Cluster {row[\"cluster\"]}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    \"\"\"# Add markers for cluster centers\n",
    "    for i, center in cluster_centers.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[center['latitude'], center['longitude']],\n",
    "            icon=folium.Icon(color='black', icon='info-sign'),\n",
    "            popup=f'Cluster Center {i}'\n",
    "        ).add_to(m)\"\"\"\n",
    "    display(m)\n",
    "\n",
    "stations_clusters_on_map(clustered_stations_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e158d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# SUMMED DEMAND HEATMAP (cell 19)\n",
    "\n",
    "# sum total demand per cluster across the dataset\n",
    "cluster_total = df_demand.groupby('cluster')['demand'].sum().reset_index()\n",
    "\n",
    "cluster_centers = clustered_stations_kmeans.groupby('cluster')[['latitude', 'longitude']].mean()\n",
    "center_lat = cluster_centers['latitude'].mean()\n",
    "center_lon = cluster_centers['longitude'].mean()\n",
    "\n",
    "# merge with cluster centers (cluster_centers index corresponds to cluster id)\n",
    "cluster_centers_idx = cluster_centers.reset_index().rename(columns={'index': 'cluster'})\n",
    "cluster_total = cluster_total.merge(cluster_centers_idx, on='cluster', how='left').dropna(subset=['latitude','longitude'])\n",
    "\n",
    "# normalize sizes and colors\n",
    "max_d = cluster_total['demand'].max()\n",
    "cluster_total['radius'] = (cluster_total['demand'] / max_d) * 40 + 5  # radius in pixels\n",
    "\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(vmin=cluster_total['demand'].min(), vmax=max_d)\n",
    "\n",
    "# create folium map centered on existing center\n",
    "m_demand = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# add circle markers sized & colored by total demand\n",
    "for _, r in cluster_total.iterrows():\n",
    "    color = mcolors.to_hex(cmap(norm(r['demand'])))\n",
    "    folium.CircleMarker(\n",
    "        location=[r['latitude'], r['longitude']],\n",
    "        radius=float(r['radius']),\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Cluster {int(r['cluster'])} â€” Demand: {int(r['demand'])}\"\n",
    "    ).add_to(m_demand)\n",
    "\n",
    "# add HeatMap layer (weights proportional to demand)\n",
    "heat_data = [[r['latitude'], r['longitude'], r['demand']/max_d] for _, r in cluster_total.iterrows()]\n",
    "HeatMap(heat_data, radius=25, max_zoom=13).add_to(m_demand)\n",
    "\n",
    "m_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee718d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Cluster demand metrics & visuals (no predictions) ==================\n",
    "# Expects a DataFrame 'df' where:\n",
    "#  - index is datetime\n",
    "#  - columns: ['cluster','pickups','dropoffs','demand']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Wide matrices (time x cluster)\n",
    "def build_cluster_wide(df: pd.DataFrame, value_col: str = \"demand\") -> pd.DataFrame:\n",
    "    w = df.pivot_table(index=df.index, columns=\"cluster\", values=value_col, aggfunc=\"sum\", fill_value=0)\n",
    "    # sort columns numerically if possible\n",
    "    try:\n",
    "        w = w.reindex(sorted(w.columns, key=lambda x: float(x)), axis=1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return w\n",
    "\n",
    "wide_demand  = build_cluster_wide(df_demand, \"demand\")\n",
    "wide_pickups = build_cluster_wide(df_demand, \"pickups\")\n",
    "wide_dropoff = build_cluster_wide(df_demand, \"dropoffs\")\n",
    "\n",
    "# 2) Per-cluster metrics (level, variability, activity, share, seasonality proxy)\n",
    "def _seasonal_lag(idx: pd.DatetimeIndex) -> int:\n",
    "    if len(idx) > 1:\n",
    "        step_h = np.median(np.diff(idx.values).astype(\"timedelta64[h]\").astype(float))\n",
    "        if step_h <= 1.5:  # hourly cadence\n",
    "            return 24\n",
    "    return 7\n",
    "\n",
    "def cluster_demand_metrics(wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    lag = _seasonal_lag(wide.index)\n",
    "    rows = []\n",
    "    total_all = wide.sum().sum()\n",
    "    for c in wide.columns:\n",
    "        s = wide[c].astype(float)\n",
    "        total = s.sum()\n",
    "        rows.append({\n",
    "            \"cluster\": c,\n",
    "            \"mean\": s.mean(),\n",
    "            \"std\": s.std(ddof=1),\n",
    "            \"total\": total,\n",
    "            \"peak\": s.max(),\n",
    "            \"active_periods\": int((s > 0).sum()),\n",
    "            \"share_of_total_%\": (total / (total_all + 1e-12)) * 100.0,\n",
    "            f\"autocorr_lag{lag}\": float(s.autocorr(lag)) if len(s) > lag else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"total\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "metrics = cluster_demand_metrics(wide_demand)\n",
    "display(metrics.head(15))\n",
    "\n",
    "# 3) Correlation heatmap (how clusters co-move over time)\n",
    "def plot_cluster_corr_heatmap(wide: pd.DataFrame, method: str = \"pearson\", title_suffix: str = \"\"):\n",
    "    corr = wide.corr(method=method)\n",
    "    fig = plt.figure(figsize=(8, 7)); ax = plt.gca()\n",
    "    im = ax.imshow(corr.values, aspect=\"auto\")\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04); cbar.set_label(f\"{method.title()} correlation\")\n",
    "    labels = [str(int(c)) if isinstance(c, (int, np.integer, float)) and float(c).is_integer() else str(c)\n",
    "              for c in corr.columns]\n",
    "    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90); ax.set_yticklabels(labels)\n",
    "    ax.set_title(f\"Cluster demand correlation ({method}) {title_suffix}\")\n",
    "    ax.set_xlabel(\"Cluster\"); ax.set_ylabel(\"Cluster\")\n",
    "    plt.tight_layout()\n",
    "    return corr\n",
    "\n",
    "corr_pearson = plot_cluster_corr_heatmap(wide_demand, method=\"pearson\")\n",
    "plt.show()\n",
    "\n",
    "# 4) Hour-of-day profile heatmap (avg demand by hour & cluster)\n",
    "def plot_hourly_profile_heatmap(df: pd.DataFrame, value_col: str = \"demand\"):\n",
    "    g = df.copy()\n",
    "    g[\"hour\"] = g.index.hour\n",
    "    prof = g.pivot_table(index=\"hour\", columns=\"cluster\", values=value_col, aggfunc=\"mean\", fill_value=0)\n",
    "    # keep columns order similar to wide\n",
    "    try:\n",
    "        prof = prof.reindex(sorted(prof.columns, key=lambda x: float(x)), axis=1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    fig = plt.figure(figsize=(10, 5)); ax = plt.gca()\n",
    "    im = ax.imshow(prof.values, aspect=\"auto\", origin=\"lower\")\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04); cbar.set_label(f\"Avg {value_col}\")\n",
    "    ax.set_yticks(range(0, 24)); ax.set_yticklabels(range(0, 24))\n",
    "    ax.set_xlabel(\"Cluster\"); ax.set_ylabel(\"Hour of day\")\n",
    "    ax.set_xticks(range(prof.shape[1]))\n",
    "    ax.set_xticklabels([str(int(c)) if isinstance(c,(int,np.integer,float)) and float(c).is_integer() else str(c)\n",
    "                        for c in prof.columns], rotation=90)\n",
    "    ax.set_title(f\"Hourly average {value_col} by cluster\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_hourly_profile_heatmap(df_demand, value_col=\"demand\")\n",
    "plt.show()\n",
    "\n",
    "# 5) Share-of-total bar chart (who contributes most to system demand)\n",
    "def plot_cluster_total_share(metrics_df: pd.DataFrame, title_suffix=\"\"):\n",
    "    m = metrics_df.sort_values(\"total\", ascending=False)\n",
    "    x = np.arange(len(m))\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(x, m[\"total\"].values)\n",
    "    plt.xticks(x, [str(int(c)) if isinstance(c,(int,np.integer,float)) and float(c).is_integer() else str(c)\n",
    "                   for c in m[\"cluster\"]], rotation=0)\n",
    "    plt.ylabel(\"Total demand\"); plt.xlabel(\"Cluster\")\n",
    "    plt.title(f\"Total demand by cluster {title_suffix}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_cluster_total_share(metrics)\n",
    "plt.show()\n",
    "\n",
    "# 6) Optional: top co-moving cluster pairs (based on Pearson corr on demand)\n",
    "def top_corr_pairs(corr: pd.DataFrame, k: int = 10) -> pd.DataFrame:\n",
    "    c = corr.copy(); np.fill_diagonal(c.values, np.nan)\n",
    "    pairs = []\n",
    "    cols = c.columns.to_list()\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            pairs.append((cols[i], cols[j], c.iloc[i, j]))\n",
    "    out = pd.DataFrame(pairs, columns=[\"cluster_i\",\"cluster_j\",\"corr\"]).sort_values(\"corr\", ascending=False)\n",
    "    return out.head(k)\n",
    "\n",
    "display(top_corr_pairs(corr_pearson, k=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4473ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pickup (cluster i) vs Dropoff (cluster j) correlation ===\n",
    "# Expects df with index=datetime and columns: ['cluster','pickups','dropoffs','demand']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _build_wide(df: pd.DataFrame, value_col: str) -> pd.DataFrame:\n",
    "    w = df.pivot_table(index=df.index, columns=\"cluster\", values=value_col, aggfunc=\"sum\", fill_value=0)\n",
    "    try:\n",
    "        w = w.reindex(sorted(w.columns, key=lambda x: float(x)), axis=1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return w\n",
    "\n",
    "def pickup_dropoff_corr_matrix(df: pd.DataFrame, lag_hours: int = 0, method: str = \"pearson\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Correlate pickups at cluster i with dropoffs at cluster j.\n",
    "    lag_hours>0 means dropoffs occur later; we align pickups(t) with dropoffs(t+lag).\n",
    "    \"\"\"\n",
    "    P = _build_wide(df, \"pickups\")\n",
    "    D = _build_wide(df, \"dropoffs\")\n",
    "\n",
    "    if lag_hours != 0:\n",
    "        # align pickups(t) with dropoffs(t+lag)\n",
    "        D = D.shift(-lag_hours)\n",
    "\n",
    "    # align time index\n",
    "    common = P.index.intersection(D.index)\n",
    "    P, D = P.loc[common], D.loc[common]\n",
    "\n",
    "    # drop constant columns (std=0) to avoid NaNs-only columns\n",
    "    P = P.loc[:, P.std(ddof=0) > 0]\n",
    "    D = D.loc[:, D.std(ddof=0) > 0]\n",
    "\n",
    "    # compute matrix: rows = pickup clusters, cols = dropoff clusters\n",
    "    mat = pd.DataFrame(index=P.columns, columns=D.columns, dtype=float)\n",
    "    for dcol in D.columns:\n",
    "        mat[dcol] = P.corrwith(D[dcol], axis=0, method=method)\n",
    "    return mat\n",
    "\n",
    "def plot_pickup_dropoff_corr_heatmap(corr: pd.DataFrame, title_suffix: str = \"\"):\n",
    "    fig = plt.figure(figsize=(9, 7)); ax = plt.gca()\n",
    "    im = ax.imshow(corr.values, aspect=\"auto\")\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Correlation\")\n",
    "\n",
    "    xt = [str(int(c)) if isinstance(c,(int,np.integer,float)) and float(c).is_integer() else str(c) for c in corr.columns]\n",
    "    yt = [str(int(r)) if isinstance(r,(int,np.integer,float)) and float(r).is_integer() else str(r) for r in corr.index]\n",
    "    ax.set_xticks(range(len(xt))); ax.set_yticks(range(len(yt)))\n",
    "    ax.set_xticklabels(xt, rotation=90); ax.set_yticklabels(yt)\n",
    "    ax.set_xlabel(\"Dropoff cluster (j)\"); ax.set_ylabel(\"Pickup cluster (i)\")\n",
    "    ax.set_title(f\"Corr[pickups_i, dropoffs_j] {title_suffix}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "# ==== Run (examples) ====\n",
    "# corr_0h: same-hour correlation\n",
    "corr_0h = pickup_dropoff_corr_matrix(df_demand, lag_hours=0, method=\"pearson\")\n",
    "plot_pickup_dropoff_corr_heatmap(corr_0h, title_suffix=\"(lag=0h)\")\n",
    "plt.show()\n",
    "\n",
    "# corr_2h: pickups(t) vs dropoffs(t+2h) correlation (helpful if effects are delayed)\n",
    "corr_2h = pickup_dropoff_corr_matrix(df_demand, lag_hours=2, method=\"pearson\")\n",
    "plot_pickup_dropoff_corr_heatmap(corr_2h, title_suffix=\"(lag=2h)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece7a1d",
   "metadata": {},
   "source": [
    "Hereâ€™s a tight, battle-tested strategy for Task 2 that fits your setup (clusters already fixed, hourly data, Janâ€“Oct train / Novâ€“Dec test).\n",
    "\n",
    "1) What to predict & how to structure the data\n",
    "\n",
    "Targets (per cluster c):\n",
    "\n",
    "pickups_c(t) for each hour of Day+1 (24 steps)\n",
    "\n",
    "dropoffs_c(t) for each hour of Day+1 (24 steps)\n",
    "\n",
    "Data you should keep (both!)\n",
    "\n",
    "Cluster-level time series: datetime, cluster, pickups, dropoffs, demand\n",
    "\n",
    "OD table (you already have): start_cluster, end_cluster, start_datetime_hour, stop_datetime_hour\n",
    "\n",
    "Rationale: cluster series are perfect for univariate/seasonal models; OD lets you model dropoffs as a function of earlier pickups elsewhere (travel). Keep both paths available.\n",
    "\n",
    "2) Baselines (must-have yardsticks)\n",
    "\n",
    "Seasonal naÃ¯ve (24h): \n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘¡\n",
    "+\n",
    "â„\n",
    "=\n",
    "ğ‘¦\n",
    "ğ‘¡\n",
    "+\n",
    "â„\n",
    "âˆ’\n",
    "24\n",
    "y\n",
    "^\n",
    "\tâ€‹\n",
    "\n",
    "t+h\n",
    "\tâ€‹\n",
    "\n",
    "=y\n",
    "t+hâˆ’24\n",
    "\tâ€‹\n",
    "\n",
    "\n",
    "Weekly seasonal (168h): \n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘¡\n",
    "+\n",
    "â„\n",
    "=\n",
    "ğ‘¦\n",
    "ğ‘¡\n",
    "+\n",
    "â„\n",
    "âˆ’\n",
    "168\n",
    "y\n",
    "^\n",
    "\tâ€‹\n",
    "\n",
    "t+h\n",
    "\tâ€‹\n",
    "\n",
    "=y\n",
    "t+hâˆ’168\n",
    "\tâ€‹\n",
    "\n",
    "\n",
    "ETS (Holt-Winters) per cluster for quick strong baseline.\n",
    "\n",
    "Use them for both pickups & dropoffs; never skip baselines.\n",
    "\n",
    "3) Pickups model (per cluster)\n",
    "\n",
    "Pickups are largely cluster-local + strong daily/weekly seasonality. Two robust options:\n",
    "\n",
    "A) SARIMA / ExponentialSmoothing (per cluster)\n",
    "\n",
    "Train per cluster on Janâ€“Oct.\n",
    "\n",
    "24-step direct forecast at midnight (multi-step from the fitted model).\n",
    "\n",
    "Good when signal is seasonal/stationary; quick & dependable.\n",
    "\n",
    "B) Gradient-boosted trees (XGBoost/LightGBM) with lags\n",
    "\n",
    "Features: hour, dow, month, pickups_lag{1,24,168}, rolling{24,168}, optional: dropoffs_lag{â€¦}.\n",
    "\n",
    "Direct multi-output: either recursive (predict hour h then feed forward) or 24 small â€œdirectâ€ models.\n",
    "\n",
    "Strong when seasonality + nonlinear effects; fast to iterate.\n",
    "\n",
    "ğŸ‘‰ Practical plan: train both per cluster; keep the best per-cluster (MAE/WAPE on Novâ€“Dec). In many bike datasets, ETS/SARIMA wins for quieter clusters, XGB wins for busier/noisier ones.\n",
    "\n",
    "4) Dropoffs model (per cluster): two levels\n",
    "\n",
    "Dropoffs at cluster d are the result of earlier pickups at origins o with travel time. Use the OD table.\n",
    "\n",
    "Level 1 â€” OD kernel (recommended):\n",
    "Learn a transition kernel \n",
    "ğ¾\n",
    "ğ‘œ\n",
    "â†’\n",
    "ğ‘‘\n",
    "(\n",
    "ğœ\n",
    ")\n",
    "K\n",
    "oâ†’d\n",
    "\tâ€‹\n",
    "\n",
    "(Ï„) = P(dropoff at d occurs \n",
    "ğœ\n",
    "Ï„ hours after a pickup at o) from Janâ€“Oct:\n",
    "\n",
    "Build histograms of Ï„ = stop_hour âˆ’ start_hour per (o,d), normalize across d,Ï„.\n",
    "\n",
    "(Optionally) condition on hour-of-day / weekday for better realism.\n",
    "\n",
    "Predicting Day+1 dropoffs: convolve your predicted pickups with \n",
    "ğ¾\n",
    "K:\n",
    "\n",
    "ğ‘‘\n",
    "ğ‘Ÿ\n",
    "ğ‘œ\n",
    "ğ‘\n",
    "ğ‘œ\n",
    "ğ‘“\n",
    "ğ‘“\n",
    "ğ‘ \n",
    "^\n",
    "ğ‘‘\n",
    "(\n",
    "ğ‘¡\n",
    ")\n",
    "=\n",
    "âˆ‘\n",
    "ğ‘œ\n",
    "âˆ‘\n",
    "ğœ\n",
    "ğ‘\n",
    "ğ‘–\n",
    "ğ‘\n",
    "ğ‘˜\n",
    "ğ‘¢\n",
    "ğ‘\n",
    "ğ‘ \n",
    "^\n",
    "ğ‘œ\n",
    "(\n",
    "ğ‘¡\n",
    "âˆ’\n",
    "ğœ\n",
    ")\n",
    "â€‰\n",
    "ğ¾\n",
    "ğ‘œ\n",
    "â†’\n",
    "ğ‘‘\n",
    "(\n",
    "ğœ\n",
    ")\n",
    "dropoffs\n",
    "\tâ€‹\n",
    "\n",
    "d\n",
    "\tâ€‹\n",
    "\n",
    "(t)=\n",
    "o\n",
    "âˆ‘\n",
    "\tâ€‹\n",
    "\n",
    "Ï„\n",
    "âˆ‘\n",
    "\tâ€‹\n",
    "\n",
    "pickups\n",
    "\tâ€‹\n",
    "\n",
    "o\n",
    "\tâ€‹\n",
    "\n",
    "(tâˆ’Ï„)K\n",
    "oâ†’d\n",
    "\tâ€‹\n",
    "\n",
    "(Ï„)\n",
    "\n",
    "Pros: causal, interpretable, uses OD you already have.\n",
    "Calibrate smoothing: shrink tiny OD pairs, cap Ï„ (e.g., 0â€“6h window).\n",
    "\n",
    "Level 2 â€” Local refinement (optional):\n",
    "Per-cluster SARIMAX with exogenous terms = lagged local pickups and/or inbound pickups from top origins (from OD).\n",
    "This soaks up residual seasonality the kernel misses.\n",
    "\n",
    "Fallback if you skip OD: per-cluster SARIMAX with exogenous = own pickups_lag{1,2,â€¦}, or a VAR( [pickups, dropoffs] ) per cluster. Works, but gives up spatial causality.\n",
    "\n",
    "5) Train / validation protocol (strictly time-aware)\n",
    "\n",
    "Train: 2018-01-01 â€¦ 2018-10-31 23:00\n",
    "\n",
    "Test: 2018-11-01 â€¦ 2018-12-31\n",
    "\n",
    "For every test day D, fit/retain your model on data â‰¤ D-1 23:00, then predict Dâ€™s 24 hours.\n",
    "\n",
    "Keep same split for all clusters, no shuffling.\n",
    "\n",
    "6) Features to prepare (minimal â†’ strong)\n",
    "\n",
    "Always: hour, day_of_week, month.\n",
    "\n",
    "Lags & rolls (per target): lag{1,24,168}, rolling mean/std {24,168}.\n",
    "\n",
    "For OD kernel: counts of flows per (o,d,Ï„); store as a sparse mapping or small arrays per (o,d).\n",
    "\n",
    "Optional extras (if you later add them): weather (temp, precip), holiday flags.\n",
    "\n",
    "Schema answers to your question:\n",
    "\n",
    "Keep both:\n",
    "\n",
    "timeâ€“clusterâ€“pickups/dropoffs (for per-cluster models/metrics) and\n",
    "\n",
    "timeâ€“start_cluster (pickups) plus timeâ€“end_cluster (dropoffs) via your OD table (for the kernel).\n",
    "They serve different modeling steps and you need both for the best dropoff predictions.\n",
    "\n",
    "7) Metrics to decide â€œwhat winsâ€\n",
    "\n",
    "Report per cluster and overall (Novâ€“Dec):\n",
    "\n",
    "MAE, RMSE, WAPE, sMAPE, MASE(24), Directional Accuracy.\n",
    "\n",
    "Hour-of-day MAE profiles (where it struggles).\n",
    "\n",
    "For dropoffs, also OD distribution fidelity (e.g., Jensenâ€“Shannon between true vs predicted destination share from each origin).\n",
    "\n",
    "8) Minimal implementation sketch (pseudo-code)\n",
    "\n",
    "Pickups per cluster\n",
    "\n",
    "for cluster c:\n",
    "    fit SARIMA and XGB on Janâ€“Oct\n",
    "    for each test day D:\n",
    "        Å·_pu_c[Dâ€™s 24h] = model.predict_24h(history â‰¤ D-1 23:00)\n",
    "    choose best model per-cluster by WAPE\n",
    "\n",
    "\n",
    "OD kernel for dropoffs\n",
    "\n",
    "# TRAIN (Janâ€“Oct)\n",
    "for each (o,d):\n",
    "    histogram Ï„ = stop_hour - start_hour  within Ï„ âˆˆ [0..6] (example)\n",
    "    normalize over d,Ï„ with smoothing -> K[o,d,Ï„]\n",
    "\n",
    "# PREDICT Day D\n",
    "for each destination d and hour t âˆˆ D:\n",
    "    Å·_do_d(t) = sum_o sum_Ï„ Å·_pu_o(t-Ï„) * K[o,d,Ï„]\n",
    "\n",
    "\n",
    "(Optional) Local SARIMAX refinement on residuals.\n",
    "\n",
    "9) Why this split of responsibilities works\n",
    "\n",
    "Pickups are â€œsupply leavingâ€ decisions localized in spaceâ€“time â†’ per-cluster seasonal models excel.\n",
    "\n",
    "Dropoffs are consequences of earlier pickups elsewhere with a travel-time kernel â†’ the OD convolution is the most faithful and usually beats standalone univariate models for dropoffs.\n",
    "\n",
    "If you want, I can drop in ready-to-paste notebook cells for:\n",
    "\n",
    "building the OD kernel \n",
    "ğ¾\n",
    "ğ‘œ\n",
    "â†’\n",
    "ğ‘‘\n",
    "(\n",
    "ğœ\n",
    ")\n",
    "K\n",
    "oâ†’d\n",
    "\tâ€‹\n",
    "\n",
    "(Ï„) from your table,\n",
    "\n",
    "the day-ahead convolution using your existing pickups forecasts, and\n",
    "\n",
    "the evaluation tables/plots (WAPE per cluster, OD heatmaps of error)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
